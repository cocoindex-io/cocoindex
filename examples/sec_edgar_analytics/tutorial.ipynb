{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEC EDGAR Financial Analytics\n",
    "\n",
    "## Building a Compliance Search Tool with CocoIndex + Apache Doris\n",
    "\n",
    "---\n",
    "\n",
    "### The Problem\n",
    "\n",
    "You're a fintech company building a **compliance search tool**. Your analysts need to quickly find relevant risk disclosures across thousands of SEC filings:\n",
    "\n",
    "> *\"Show me all cybersecurity risk mentions from tech companies in the last year, ranked by relevance.\"*\n",
    "\n",
    "This sounds simple, but production requirements are complex:\n",
    "\n",
    "| Requirement | Why It's Hard |\n",
    "|-------------|---------------|\n",
    "| **Semantic search** | \"data breach\" should match \"unauthorized access to systems\" |\n",
    "| **Keyword precision** | When analysts search \"GDPR\", they want exact matches |\n",
    "| **Temporal awareness** | 2024 filings matter more than 2019 filings |\n",
    "| **Category filtering** | Filter by risk type (cyber, climate, regulatory) |\n",
    "| **Multi-format data** | Filings (TXT), financial facts (JSON), exhibits (PDF) |\n",
    "| **Audit trail** | Know exactly which document each result came from |\n",
    "| **Incremental updates** | Daily SEC filings shouldn't require full reprocessing |\n",
    "\n",
    "This tutorial shows you how to build it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What You'll Build\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│                        Compliance Search Tool                           │\n",
    "├─────────────────────────────────────────────────────────────────────────┤\n",
    "│  Query: \"cybersecurity risks\"                                           │\n",
    "│  Filters: time_gate=365 days, topics=[RISK:CYBER], source=filing        │\n",
    "│                                                                         │\n",
    "│  Results:                                                               │\n",
    "│  [0.032] Apple 10-K 2024 → \"We face significant cybersecurity...\"       │\n",
    "│  [0.029] Microsoft 10-K 2024 → \"Cyber threats continue to evolve...\"    │\n",
    "│  [0.025] JPMorgan 10-K 2024 → \"We invest $700M in cybersecurity...\"     │\n",
    "└─────────────────────────────────────────────────────────────────────────┘\n",
    "                                    ▲\n",
    "                                    │ Hybrid Search\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│                          Apache Doris                                   │\n",
    "│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐                   │\n",
    "│  │ HNSW Vector  │  │  Inverted    │  │   Array      │                   │\n",
    "│  │    Index     │  │   Index      │  │  Columns     │                   │\n",
    "│  │ (semantic)   │  │ (keywords)   │  │ (topics[])   │                   │\n",
    "│  └──────────────┘  └──────────────┘  └──────────────┘                   │\n",
    "└─────────────────────────────────────────────────────────────────────────┘\n",
    "                                    ▲\n",
    "                                    │ Incremental ETL\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│                     CocoIndex Multi-Source Pipeline                     │\n",
    "│                                                                         │\n",
    "│  ┌──────────────┐   ┌──────────────┐   ┌──────────────┐                 │\n",
    "│  │ TXT Filings  │   │ JSON Facts   │   │ PDF Exhibits │                 │\n",
    "│  │ (10-K/10-Q)  │   │ (API Data)   │   │ (Documents)  │                 │\n",
    "│  └──────┬───────┘   └──────┬───────┘   └──────┬───────┘                 │\n",
    "│         │                  │                  │                         │\n",
    "│         ▼                  ▼                  ▼                         │\n",
    "│  ┌─────────────────────────────────────────────────────┐                │\n",
    "│  │              Unified Chunk Collector                │                │\n",
    "│  │   Scrub PII → Chunk → Embed → Extract Topics        │                │\n",
    "│  └─────────────────────────────────────────────────────┘                │\n",
    "│                                                                         │\n",
    "│  ✓ Cached (only reprocess changed files)                                │\n",
    "│  ✓ Incremental (daily updates in seconds)                               │\n",
    "│  ✓ Traceable (full lineage from chunk to source)                        │\n",
    "└─────────────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Why Hybrid Search?\n",
    "\n",
    "Let's understand why we need both vector and keyword search.\n",
    "\n",
    "### The Semantic Gap\n",
    "\n",
    "An analyst searches for **\"cybersecurity risks\"**:\n",
    "\n",
    "```\n",
    "Keyword search alone:\n",
    "✗ MISSES: \"We experienced unauthorized access to our systems...\"\n",
    "✗ MISSES: \"Hackers exploited a vulnerability in our network...\"\n",
    "✓ FINDS:  \"cybersecurity risks continue to threaten...\"\n",
    "```\n",
    "\n",
    "Vector embeddings capture semantic meaning — \"unauthorized access\" is conceptually similar to \"cybersecurity risks\".\n",
    "\n",
    "### The Precision Problem\n",
    "\n",
    "But vector search alone has issues:\n",
    "\n",
    "```\n",
    "Query: \"GDPR compliance\"\n",
    "\n",
    "Vector search returns:\n",
    "✗ \"We comply with various privacy regulations...\" (too vague)\n",
    "✗ \"Data protection is important to us...\" (no GDPR mention)\n",
    "✓ \"GDPR requires us to...\" (what analyst wants)\n",
    "```\n",
    "\n",
    "When analysts use specific terms like \"GDPR\" or \"SOX\", they expect exact matches.\n",
    "\n",
    "### The Solution: Hybrid Search with RRF\n",
    "\n",
    "We combine both signals using **Reciprocal Rank Fusion (RRF)**:\n",
    "\n",
    "```\n",
    "rrf_score = 1/(k + semantic_rank) + 1/(k + lexical_rank)\n",
    "```\n",
    "\n",
    "- k=60 is the standard constant (no tuning needed)\n",
    "- Used by Elasticsearch, Azure Cognitive Search\n",
    "- Handles different score scales gracefully"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Setup\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- Python 3.11+\n",
    "- Docker (for Apache Doris)\n",
    "- ~5 minutes for setup\n",
    "\n",
    "### Start the Infrastructure\n",
    "\n",
    "```bash\n",
    "# Start Doris and PostgreSQL (CocoIndex metadata store)\n",
    "docker compose up -d\n",
    "\n",
    "# Wait for Doris to be ready (~60 seconds)\n",
    "docker compose logs -f doris-fe\n",
    "# Look for: \"Doris FE started successfully\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment ready!\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "\n",
    "# CocoIndex - ETL framework\n",
    "import cocoindex\n",
    "import cocoindex.targets.doris as coco_doris\n",
    "\n",
    "# Load environment\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Environment ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doris: localhost:9030\n"
     ]
    }
   ],
   "source": [
    "# Doris configuration (defaults work with docker-compose.yml)\n",
    "DORIS_CONFIG = {\n",
    "    \"fe_host\": os.environ.get(\"DORIS_FE_HOST\", \"localhost\"),\n",
    "    \"fe_http_port\": int(os.environ.get(\"DORIS_HTTP_PORT\", \"8030\")),\n",
    "    \"query_port\": int(os.environ.get(\"DORIS_QUERY_PORT\", \"9030\")),\n",
    "    \"username\": os.environ.get(\"DORIS_USERNAME\", \"root\"),\n",
    "    \"password\": os.environ.get(\"DORIS_PASSWORD\", \"\"),\n",
    "    \"database\": os.environ.get(\"DORIS_DATABASE\", \"sec_analytics\"),\n",
    "}\n",
    "\n",
    "TABLE_CHUNKS = \"filing_chunks\"\n",
    "\n",
    "print(f\"Doris: {DORIS_CONFIG['fe_host']}:{DORIS_CONFIG['query_port']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: The Data\n",
    "\n",
    "SEC provides data in **three different formats**, each serving different needs:\n",
    "\n",
    "### 3.1 TXT Filings (10-K, 10-Q, 8-K)\n",
    "\n",
    "Plain text annual/quarterly reports containing:\n",
    "- **Item 1A: Risk Factors** — What could go wrong\n",
    "- **Item 7: MD&A** — Management's analysis\n",
    "- **Item 8: Financial Statements** — The numbers\n",
    "\n",
    "```\n",
    "data/filings/0000320193_2024-11-01_10-K.txt\n",
    "             │          │          │\n",
    "             │          │          └── Form type\n",
    "             │          └───────────── Filing date\n",
    "             └──────────────────────── CIK (company ID)\n",
    "```\n",
    "\n",
    "### 3.2 JSON Company Facts (SEC API)\n",
    "\n",
    "Structured financial data from SEC's XBRL API:\n",
    "- Revenue, net income, assets\n",
    "- Time series data (multiple years)\n",
    "- Machine-readable metrics\n",
    "\n",
    "```\n",
    "data/company_facts/0000320193.json\n",
    "```\n",
    "\n",
    "### 3.3 PDF Exhibits\n",
    "\n",
    "Supporting documents attached to filings:\n",
    "- Executive contracts\n",
    "- Legal agreements\n",
    "- Technical specifications\n",
    "\n",
    "```\n",
    "data/exhibits_pdf/0000320193_2024-ex21.pdf\n",
    "```\n",
    "\n",
    "**This tutorial shows how to ingest ALL THREE formats into a single searchable index.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample SEC data\n",
    "from download import create_sample_data\n",
    "\n",
    "create_sample_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: The Transformation Pipeline\n",
    "\n",
    "Before data becomes searchable, it passes through a series of transformations. Each step serves a specific purpose:\n",
    "\n",
    "```\n",
    "Raw Data → Metadata → Clean Text → Chunks → Embeddings + Topics → Index\n",
    "```\n",
    "\n",
    "### Transformation Overview\n",
    "\n",
    "| Step | Function | Input → Output | Why It's Needed |\n",
    "|------|----------|---------------|-----------------|\n",
    "| **1. Metadata Extraction** | `extract_*_metadata()` | filename → struct | Enable filtering by CIK, date, form type |\n",
    "| **2. Content Parsing** | `parse_company_facts()` | JSON → text | Make structured data searchable |\n",
    "| **3. PII Scrubbing** | `scrub_pii()` | text → text | Remove emails, phones, SSNs |\n",
    "| **4. Chunking** | `SplitRecursively()` | text → chunks[] | Create searchable units |\n",
    "| **5. Embedding** | `text_to_embedding()` | text → vector | Enable semantic search |\n",
    "| **6. Topic Tagging** | `extract_topics()` | text → topics[] | Enable category filtering |\n",
    "\n",
    "### CocoIndex Function Patterns\n",
    "\n",
    "Functions are defined in `functions.py` using the `@cocoindex.op.function` decorator:\n",
    "\n",
    "```python\n",
    "@cocoindex.op.function(cache=True, behavior_version=1)\n",
    "def my_transform(input: str) -> Output:\n",
    "    # cache=True: Results are cached, only recompute on input change\n",
    "    # behavior_version: Bump this to invalidate cache when logic changes\n",
    "    ...\n",
    "```\n",
    "\n",
    "This enables **incremental processing** — when you add new filings, only the new files are processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformations loaded from functions.py\n"
     ]
    }
   ],
   "source": [
    "# Import all transformations from functions.py\n",
    "# Keeping them in a separate file avoids \"Function factory already exists\" errors\n",
    "# on notebook re-runs (Python caches the module, so decorators only execute once).\n",
    "from functions import (\n",
    "    # Data classes (return types)\n",
    "    extract_filing_metadata,  # filename → FilingMetadata\n",
    "    extract_json_metadata,  # (filename, content) → CompanyFactsMetadata\n",
    "    parse_company_facts,  # JSON content → searchable text\n",
    "    extract_pdf_metadata,  # filename → PdfMetadata\n",
    "    scrub_pii,  # text → text (with PII removed)\n",
    "    extract_topics,  # text → list[str] (topic tags)\n",
    "    text_to_embedding,  # text → vector (384-dim)\n",
    ")\n",
    "\n",
    "print(\"Transformations loaded from functions.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Metadata Extraction\n",
    "\n",
    "Each data source encodes metadata in its filename. We extract this into structured fields for filtering:\n",
    "\n",
    "```\n",
    "0000320193_2024-11-01_10-K.txt\n",
    "│          │          │\n",
    "│          │          └── form_type: \"10-K\" (annual report)\n",
    "│          └───────────── filing_date: \"2024-11-01\"\n",
    "└──────────────────────── cik: \"0000320193\" (Apple's SEC ID)\n",
    "```\n",
    "\n",
    "**Why this matters**: Enables queries like \"show 10-K filings from 2024\" without scanning content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIK:         0000320193 (Apple Inc.)\n",
      "Filing Date: 2024-11-01\n",
      "Form Type:   10-K\n",
      "Fiscal Year: 2024\n"
     ]
    }
   ],
   "source": [
    "# Test it\n",
    "result = extract_filing_metadata(\"0000320193_2024-11-01_10-K.txt\")\n",
    "print(f\"CIK:         {result.cik} (Apple Inc.)\")\n",
    "print(f\"Filing Date: {result.filing_date}\")\n",
    "print(f\"Form Type:   {result.form_type}\")\n",
    "print(f\"Fiscal Year: {result.fiscal_year}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Content Parsing (JSON → Searchable Text)\n",
    "\n",
    "JSON company facts are structured data — great for machines, but not searchable by text queries.\n",
    "\n",
    "**The problem**: A search for \"Apple revenue\" won't match `{\"Revenues\": {\"val\": 394328000000}}`.\n",
    "\n",
    "**The solution**: `parse_company_facts()` converts JSON into natural language:\n",
    "\n",
    "```\n",
    "Input:  {\"facts\": {\"us-gaap\": {\"Revenues\": {\"val\": 394328000000, \"end\": \"2024-09-28\"}}}}\n",
    "Output: \"### Revenues\\nRecent values: 2024-09-28: $394.3B\"\n",
    "```\n",
    "\n",
    "Now the content can be chunked, embedded, and searched like any text document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata:\n",
      "  Entity: APPLE INC\n",
      "  CIK: 0000320193\n",
      "  Latest Date: 2024-11-01\n",
      "\n",
      "Parsed Content (first 800 chars):\n",
      "--------------------------------------------------\n",
      "# Company Financial Facts: APPLE INC\n",
      "CIK: 320193\n",
      "\n",
      "## US-GAAP Financial Metrics\n",
      "\n",
      "### Revenues\n",
      "Amount of revenue recognized from goods sold, services rendered.\n",
      "Recent values: 2024-09-28: $394.3B, 2023-09-30: $383.3B, 2022-09-24: $394.3B\n",
      "\n",
      "### Net Income (Loss)\n",
      "The portion of profit or loss for the period.\n",
      "Recent values: 2024-09-28: $93.7B, 2023-09-30: $97.0B\n",
      "\n",
      "### Assets\n",
      "Sum of the carrying amounts as of the balance sheet date.\n",
      "Recent values: 2024-09-28: $365.0B\n",
      "\n",
      "### Research and Development Expense\n",
      "The aggregate costs incurred for research and development.\n",
      "Recent values: 2024-09-28: $29.9B\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# Test JSON parsing\n",
    "json_file = Path(\"data/company_facts/0000320193.json\")\n",
    "if json_file.exists():\n",
    "    content = json_file.read_text()\n",
    "\n",
    "    # Test metadata extraction\n",
    "    meta = extract_json_metadata(json_file.name, content)\n",
    "    print(\"Metadata:\")\n",
    "    print(f\"  Entity: {meta.entity_name}\")\n",
    "    print(f\"  CIK: {meta.cik}\")\n",
    "    print(f\"  Latest Date: {meta.filing_date}\")\n",
    "\n",
    "    # Test content parsing\n",
    "    print(\"\\nParsed Content (first 800 chars):\")\n",
    "    print(\"-\" * 50)\n",
    "    parsed = parse_company_facts(content)\n",
    "    print(parsed[:800] + \"...\")\n",
    "else:\n",
    "    print(\"No JSON file available yet - run create_sample_data() first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: PDF Processing\n",
    "\n",
    "PDFs require two transformations:\n",
    "\n",
    "1. **Metadata extraction** (`extract_pdf_metadata`): Parse filename for CIK, date, exhibit type\n",
    "2. **Content conversion** (`PdfToMarkdown`): Convert binary PDF to searchable markdown text\n",
    "\n",
    "CocoIndex's built-in `PdfToMarkdown()` handles OCR, table extraction, and layout preservation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIK:          0000320193\n",
      "Filing Date:  2024-ex21\n",
      "Exhibit Type: EXHIBIT\n",
      "Form Type:    EXHIBIT\n"
     ]
    }
   ],
   "source": [
    "# Test PDF metadata extraction\n",
    "result = extract_pdf_metadata(\"0000320193_2024-ex21.pdf\")\n",
    "print(f\"CIK:          {result.cik}\")\n",
    "print(f\"Filing Date:  {result.filing_date}\")\n",
    "print(f\"Exhibit Type: {result.exhibit_type}\")\n",
    "print(f\"Form Type:    {result.form_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: PII Scrubbing\n",
    "\n",
    "Remove personally identifiable information **before** chunking and indexing.\n",
    "\n",
    "**Why scrub first?** Once text is chunked and embedded, PII becomes part of the searchable index. Scrubbing afterward requires reprocessing everything.\n",
    "\n",
    "**Patterns removed**:\n",
    "- SSN: `123-45-6789` → `[SSN REDACTED]`\n",
    "- Phone: `(408) 996-1010` → `[PHONE REDACTED]`\n",
    "- Email: `ir@apple.com` → `[EMAIL REDACTED]`\n",
    "\n",
    "This runs early in the pipeline so PII never enters the vector index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: Contact IR at ir@apple.com or 408-996-1010\n",
      "After:  Contact IR at [EMAIL REDACTED] or [PHONE REDACTED]\n"
     ]
    }
   ],
   "source": [
    "# Test it\n",
    "test = \"Contact IR at ir@apple.com or 408-996-1010\"\n",
    "print(f\"Before: {test}\")\n",
    "print(f\"After:  {scrub_pii(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Topic Tagging\n",
    "\n",
    "Extract structured categories from unstructured text. Each chunk gets a `topics[]` array:\n",
    "\n",
    "```python\n",
    "\"We face cybersecurity risks...\" → [\"RISK:CYBER\"]\n",
    "\"Climate change affects supply chain...\" → [\"RISK:CLIMATE\", \"RISK:SUPPLY\"]\n",
    "\"Revenue grew 15%...\" → [\"TOPIC:FINANCIAL\"]\n",
    "```\n",
    "\n",
    "**Why arrays?** Doris supports JSON array columns with efficient filtering using `json_contains()`:\n",
    "\n",
    "```sql\n",
    "-- Single topic filter\n",
    "WHERE json_contains(topics, '\"RISK:CYBER\"')\n",
    "\n",
    "-- Any matching topic (OR)\n",
    "WHERE json_contains(topics, '\"RISK:CYBER\"') \n",
    "   OR json_contains(topics, '\"RISK:CLIMATE\"')\n",
    "\n",
    "-- All matching topics (AND)\n",
    "WHERE json_contains(topics, '\"RISK:CYBER\"') \n",
    "  AND json_contains(topics, '\"RISK:REGULATORY\"')\n",
    "```\n",
    "\n",
    "This enables **faceted search** — filter by category, then rank by relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: We face significant cybersecurity risks including ...\n",
      "Topics: ['RISK:CYBER', 'RISK:REGULATORY']\n",
      "\n",
      "Text: Climate change poses risks to our supply chain ope...\n",
      "Topics: ['RISK:CLIMATE', 'RISK:SUPPLY']\n",
      "\n",
      "Text: Our AI investments include machine learning infras...\n",
      "Topics: ['TOPIC:AI']\n",
      "\n",
      "Text: Revenue increased 15% to $394.3 billion with net i...\n",
      "Topics: ['TOPIC:FINANCIAL']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test it\n",
    "samples = [\n",
    "    \"We face significant cybersecurity risks including ransomware.\",\n",
    "    \"Climate change poses risks to our supply chain operations.\",\n",
    "    \"Our AI investments include machine learning infrastructure.\",\n",
    "    \"Revenue increased 15% to $394.3 billion with net income of $97 billion.\",\n",
    "]\n",
    "\n",
    "for text in samples:\n",
    "    print(f\"Text: {text[:50]}...\")\n",
    "    print(f\"Topics: {extract_topics(text)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Text Embedding\n",
    "\n",
    "Convert text chunks to 384-dimensional vectors using `sentence-transformers/all-MiniLM-L6-v2`.\n",
    "\n",
    "**Why embeddings matter**: They capture semantic meaning, not just keywords.\n",
    "\n",
    "```\n",
    "Query: \"unauthorized access to systems\"\n",
    "  ↓ embedding\n",
    "Similar to: \"cybersecurity breach\", \"data leak\", \"hacking incident\"\n",
    "```\n",
    "\n",
    "The `@cocoindex.transform_flow` decorator ensures:\n",
    "- **Same model** for indexing and querying (critical for accuracy)\n",
    "- **Efficient batching** during indexing\n",
    "- **Async support** for query-time embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: The Multi-Source ETL Pipeline\n",
    "\n",
    "Now we assemble everything into a complete pipeline that ingests **all three data formats**.\n",
    "\n",
    "### Pipeline Structure\n",
    "\n",
    "```\n",
    "┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐\n",
    "│  TXT Filings    │  │  JSON Facts     │  │  PDF Exhibits   │\n",
    "│  data/filings/  │  │  data/company_  │  │  data/exhibits_ │\n",
    "│  *.txt          │  │  facts/*.json   │  │  pdf/*.pdf      │\n",
    "└────────┬────────┘  └────────┬────────┘  └────────┬────────┘\n",
    "         │                    │                    │\n",
    "         ▼                    ▼                    ▼\n",
    "┌──────────────────────────────────────────────────────────────┐\n",
    "│                   Unified Chunk Collector                     │\n",
    "│                                                              │\n",
    "│  Each source:                                                │\n",
    "│    1. Extracts metadata (filename parsing)                   │\n",
    "│    2. Parses content (text/JSON → searchable text, PDF → md) │\n",
    "│    3. Scrubs PII                                             │\n",
    "│    4. Chunks text                                            │\n",
    "│    5. Embeds + extracts topics                               │\n",
    "│    6. Collects with source_type field                        │\n",
    "└────────────────────────────┬─────────────────────────────────┘\n",
    "                             │\n",
    "                             ▼\n",
    "┌──────────────────────────────────────────────────────────────┐\n",
    "│                      Apache Doris                            │\n",
    "│  • HNSW vector index (semantic search)                       │\n",
    "│  • Inverted index (keyword search)                           │\n",
    "│  • source_type field for filtering by format                 │\n",
    "└──────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Key pattern: Multiple sources → Single collector**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper function defined: process_and_collect()\n"
     ]
    }
   ],
   "source": [
    "def process_and_collect(doc, text_field: str, metadata, collector):\n",
    "    \"\"\"\n",
    "    Common chunk processing for all source types.\n",
    "\n",
    "    This helper encapsulates the shared ETL pattern:\n",
    "    1. Scrub PII from text\n",
    "    2. Split into chunks (1000 chars, 200 overlap)\n",
    "    3. Generate embeddings\n",
    "    4. Extract topic tags\n",
    "    5. Collect into unified index\n",
    "\n",
    "    Args:\n",
    "        doc: The document row from a source\n",
    "        text_field: Name of the field containing text to process\n",
    "        metadata: Extracted metadata (includes cik, filing_date, form_type,\n",
    "                  fiscal_year, source_type)\n",
    "        collector: The chunk collector to add results to\n",
    "    \"\"\"\n",
    "    # Scrub PII before any processing\n",
    "    doc[\"scrubbed\"] = doc[text_field].transform(scrub_pii)\n",
    "\n",
    "    # Split into searchable chunks\n",
    "    doc[\"chunks\"] = doc[\"scrubbed\"].transform(\n",
    "        cocoindex.functions.SplitRecursively(),\n",
    "        language=\"markdown\",\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "    )\n",
    "\n",
    "    # Process each chunk: embed + tag + collect\n",
    "    with doc[\"chunks\"].row() as chunk:\n",
    "        chunk[\"embedding\"] = text_to_embedding(chunk[\"text\"])\n",
    "        chunk[\"topics\"] = chunk[\"text\"].transform(extract_topics)\n",
    "\n",
    "        collector.collect(\n",
    "            chunk_id=cocoindex.GeneratedField.UUID,\n",
    "            source_type=metadata[\"source_type\"],\n",
    "            doc_filename=doc[\"filename\"],\n",
    "            location=chunk[\"location\"],\n",
    "            cik=metadata[\"cik\"],\n",
    "            filing_date=metadata[\"filing_date\"],\n",
    "            form_type=metadata[\"form_type\"],\n",
    "            fiscal_year=metadata[\"fiscal_year\"],\n",
    "            text=chunk[\"text\"],\n",
    "            embedding=chunk[\"embedding\"],\n",
    "            topics=chunk[\"topics\"],\n",
    "        )\n",
    "\n",
    "\n",
    "print(\"Helper function defined: process_and_collect()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: PdfToMarkdown not available — PDF exhibits will be skipped.\n",
      "Pipeline defined: SECFilingAnalytics\n"
     ]
    }
   ],
   "source": [
    "# Close existing flow if re-running this cell (safe no-op on first run)\n",
    "try:\n",
    "    sec_filing_flow.close()  # type: ignore[name-defined]\n",
    "except NameError:\n",
    "    pass  # First run — no flow to close\n",
    "\n",
    "# Check if PdfToMarkdown is available (requires optional dependency)\n",
    "_has_pdf = hasattr(cocoindex.functions, \"PdfToMarkdown\")\n",
    "if not _has_pdf:\n",
    "    print(\"Note: PdfToMarkdown not available — PDF exhibits will be skipped.\")\n",
    "\n",
    "\n",
    "@cocoindex.flow_def(name=\"SECFilingAnalytics\")\n",
    "def sec_filing_flow(\n",
    "    flow_builder: cocoindex.FlowBuilder, data_scope: cocoindex.DataScope\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    SEC Filing Analytics Pipeline - Multi-Source\n",
    "\n",
    "    Ingests TXT, JSON, and optionally PDF into a unified searchable index.\n",
    "    \"\"\"\n",
    "\n",
    "    # ═══════════════════════════════════════════════════════════════════\n",
    "    # SOURCES\n",
    "    # ═══════════════════════════════════════════════════════════════════\n",
    "    data_scope[\"txt_filings\"] = flow_builder.add_source(\n",
    "        cocoindex.sources.LocalFile(path=\"data/filings\", included_patterns=[\"*.txt\"]),\n",
    "        refresh_interval=timedelta(hours=1),\n",
    "    )\n",
    "    data_scope[\"json_facts\"] = flow_builder.add_source(\n",
    "        cocoindex.sources.LocalFile(\n",
    "            path=\"data/company_facts\", included_patterns=[\"*.json\"]\n",
    "        ),\n",
    "        refresh_interval=timedelta(hours=1),\n",
    "    )\n",
    "\n",
    "    # ═══════════════════════════════════════════════════════════════════\n",
    "    # UNIFIED COLLECTOR - all sources feed into this\n",
    "    # ═══════════════════════════════════════════════════════════════════\n",
    "    chunk_collector = data_scope.add_collector()\n",
    "\n",
    "    # ═══════════════════════════════════════════════════════════════════\n",
    "    # PROCESS EACH SOURCE\n",
    "    # ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "    # TXT Filings: extract metadata from filename, use content directly\n",
    "    with data_scope[\"txt_filings\"].row() as filing:\n",
    "        filing[\"metadata\"] = filing[\"filename\"].transform(extract_filing_metadata)\n",
    "        process_and_collect(filing, \"content\", filing[\"metadata\"], chunk_collector)\n",
    "\n",
    "    # JSON Facts: extract metadata (needs both filename and content),\n",
    "    # then parse JSON to searchable text\n",
    "    with data_scope[\"json_facts\"].row() as facts:\n",
    "        # Multi-input: filename is implicit first arg, content is kwarg\n",
    "        facts[\"metadata\"] = facts[\"filename\"].transform(\n",
    "            extract_json_metadata, content=facts[\"content\"]\n",
    "        )\n",
    "        facts[\"parsed\"] = facts[\"content\"].transform(parse_company_facts)\n",
    "        process_and_collect(facts, \"parsed\", facts[\"metadata\"], chunk_collector)\n",
    "\n",
    "    # PDF Exhibits: convert PDF to markdown (requires PdfToMarkdown)\n",
    "    if _has_pdf:\n",
    "        data_scope[\"pdf_exhibits\"] = flow_builder.add_source(\n",
    "            cocoindex.sources.LocalFile(\n",
    "                path=\"data/exhibits_pdf\", included_patterns=[\"*.pdf\"], binary=True\n",
    "            ),\n",
    "            refresh_interval=timedelta(hours=1),\n",
    "        )\n",
    "        with data_scope[\"pdf_exhibits\"].row() as pdf:\n",
    "            pdf[\"metadata\"] = pdf[\"filename\"].transform(extract_pdf_metadata)\n",
    "            pdf[\"markdown\"] = pdf[\"content\"].transform(\n",
    "                cocoindex.functions.PdfToMarkdown()\n",
    "            )\n",
    "            process_and_collect(pdf, \"markdown\", pdf[\"metadata\"], chunk_collector)\n",
    "\n",
    "    # ═══════════════════════════════════════════════════════════════════\n",
    "    # EXPORT TO DORIS\n",
    "    # ═══════════════════════════════════════════════════════════════════\n",
    "    chunk_collector.export(\n",
    "        \"filing_chunks\",\n",
    "        coco_doris.DorisTarget(\n",
    "            fe_host=DORIS_CONFIG[\"fe_host\"],\n",
    "            fe_http_port=DORIS_CONFIG[\"fe_http_port\"],\n",
    "            query_port=DORIS_CONFIG[\"query_port\"],\n",
    "            username=DORIS_CONFIG[\"username\"],\n",
    "            password=DORIS_CONFIG[\"password\"],\n",
    "            database=DORIS_CONFIG[\"database\"],\n",
    "            table=TABLE_CHUNKS,\n",
    "        ),\n",
    "        primary_key_fields=[\"chunk_id\"],\n",
    "        vector_indexes=[\n",
    "            cocoindex.VectorIndexDef(\n",
    "                field_name=\"embedding\",\n",
    "                metric=cocoindex.VectorSimilarityMetric.L2_DISTANCE,\n",
    "            )\n",
    "        ],\n",
    "        fts_indexes=[\n",
    "            cocoindex.FtsIndexDef(field_name=\"text\", parameters={\"parser\": \"unicode\"})\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Pipeline defined: SECFilingAnalytics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CocoIndex initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize CocoIndex\n",
    "cocoindex.init()\n",
    "print(\"CocoIndex initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up database tables...\n",
      "\u001b[2m2026-02-04T20:05:03.908582Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1msetup.apply_changes_for_flow_ctx\u001b[0m\u001b[1m{\u001b[0m\u001b[3mflow_name\u001b[0m\u001b[2m=\u001b[0mSECFilingAnalytics\u001b[1m}\u001b[0m\u001b[2m:\u001b[0m\u001b[1msetup.apply_changes_for_flow\u001b[0m\u001b[1m{\u001b[0m\u001b[3mflow_name\u001b[0m\u001b[2m=\u001b[0mSECFilingAnalytics\u001b[1m}\u001b[0m\u001b[2m:\u001b[0m \u001b[2mcocoindex_engine::execution::db_tracking_setup\u001b[0m\u001b[2m:\u001b[0m Cleaning up tracking metadata and target data for 1 stale source(s): [6]\n",
      "\u001b[2m2026-02-04T20:05:03.916399Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1msetup.apply_changes_for_flow_ctx\u001b[0m\u001b[1m{\u001b[0m\u001b[3mflow_name\u001b[0m\u001b[2m=\u001b[0mSECFilingAnalytics\u001b[1m}\u001b[0m\u001b[2m:\u001b[0m\u001b[1msetup.apply_changes_for_flow\u001b[0m\u001b[1m{\u001b[0m\u001b[3mflow_name\u001b[0m\u001b[2m=\u001b[0mSECFilingAnalytics\u001b[1m}\u001b[0m\u001b[2m:\u001b[0m \u001b[2mcocoindex_engine::execution::db_tracking_setup\u001b[0m\u001b[2m:\u001b[0m Processed 0 tracking entries, deleted 0 target rows\n",
      "\u001b[2m2026-02-04T20:05:03.917932Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1msetup.apply_changes_for_flow_ctx\u001b[0m\u001b[1m{\u001b[0m\u001b[3mflow_name\u001b[0m\u001b[2m=\u001b[0mSECFilingAnalytics\u001b[1m}\u001b[0m\u001b[2m:\u001b[0m\u001b[1msetup.apply_changes_for_flow\u001b[0m\u001b[1m{\u001b[0m\u001b[3mflow_name\u001b[0m\u001b[2m=\u001b[0mSECFilingAnalytics\u001b[1m}\u001b[0m\u001b[2m:\u001b[0m \u001b[2mcocoindex_engine::execution::db_tracking_setup\u001b[0m\u001b[2m:\u001b[0m Deleted 0 tracking entries for stale sources\n",
      "\n",
      "Processing SEC data from all sources...\n",
      "  - TXT filings (10-K/10-Q)\n",
      "  - JSON company facts\n",
      "\n",
      "(First run loads the embedding model)\n",
      "\n",
      "\n",
      "Done! Data is now searchable in Doris.\n"
     ]
    }
   ],
   "source": [
    "# Setup and run the pipeline\n",
    "print(\"Setting up database tables...\")\n",
    "await sec_filing_flow.setup_async()\n",
    "\n",
    "print(\"\\nProcessing SEC data from all sources...\")\n",
    "print(\"  - TXT filings (10-K/10-Q)\")\n",
    "print(\"  - JSON company facts\")\n",
    "if _has_pdf:\n",
    "    print(\"  - PDF exhibits\")\n",
    "print(\"\\n(First run loads the embedding model)\\n\")\n",
    "\n",
    "await sec_filing_flow.update_async()\n",
    "\n",
    "print(\"\\nDone! Data is now searchable in Doris.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Search Modes\n",
    "\n",
    "Now the fun part — searching our indexed filings! We provide **four search modes**, each optimized for different use cases:\n",
    "\n",
    "| Mode | Function | Best For | Example Query |\n",
    "|------|----------|----------|---------------|\n",
    "| **Hybrid** | `search()` | General queries | \"cybersecurity risks\" |\n",
    "| **Lexical** | `search_lexical()` | Exact terms, acronyms | \"GDPR\", \"SOX compliance\" |\n",
    "| **Topic** | `search_by_topics()` | Category filtering | \"risks\" + filter by RISK:CYBER |\n",
    "| **Portfolio** | `search_portfolio()` | Cross-company comparison | Compare Apple vs Microsoft |\n",
    "\n",
    "### How They Differ\n",
    "\n",
    "```\n",
    "Query: \"data protection requirements\"\n",
    "\n",
    "Hybrid search:\n",
    "  ✓ Finds \"GDPR compliance obligations\" (semantic match)\n",
    "  ✓ Finds \"data protection requirements\" (keyword match)\n",
    "  → Best overall results via RRF fusion\n",
    "\n",
    "Lexical search:\n",
    "  ✓ Finds \"data protection requirements\" (exact keywords)\n",
    "  ✗ Misses \"GDPR compliance obligations\" (no keyword overlap)\n",
    "  → Use when you need precise term matching\n",
    "\n",
    "Topic search:\n",
    "  ✓ Pre-filters to chunks tagged RISK:REGULATORY\n",
    "  ✓ Then ranks by semantic similarity\n",
    "  → Use when you know the category\n",
    "\n",
    "Portfolio search:\n",
    "  ✓ Returns top-K results PER company\n",
    "  → Use for cross-company analysis\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL helpers loaded from search.py\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SEARCH HELPERS (from search.py)\n",
    "# =============================================================================\n",
    "# Low-level SQL utilities are in search.py to keep the notebook focused\n",
    "# on the search logic itself.\n",
    "\n",
    "import importlib\n",
    "import search as search_mod\n",
    "\n",
    "importlib.reload(search_mod)\n",
    "\n",
    "from search import (\n",
    "    extract_keywords,\n",
    "    format_embedding,\n",
    "    format_list,\n",
    "    build_where,\n",
    "    doris_query,\n",
    ")\n",
    "\n",
    "# Common table reference\n",
    "TABLE = f\"{DORIS_CONFIG['database']}.{TABLE_CHUNKS}\"\n",
    "\n",
    "print(\"SQL helpers loaded from search.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mode 1: Hybrid Search (Recommended Default)\n",
    "\n",
    "**When to use**: General-purpose queries where you want both semantic understanding AND keyword precision.\n",
    "\n",
    "**How it works**: Reciprocal Rank Fusion (RRF) combines two rankings:\n",
    "1. **Semantic rank**: How similar is the meaning? (vector distance)\n",
    "2. **Lexical rank**: Does it contain the keywords? (inverted index)\n",
    "\n",
    "```\n",
    "RRF Score = 1/(60 + semantic_rank) + 1/(60 + lexical_rank)\n",
    "```\n",
    "\n",
    "Documents that rank high in BOTH signals get boosted to the top.\n",
    "\n",
    "```python\n",
    "# Basic hybrid search\n",
    "await search(\"cybersecurity risks\")\n",
    "\n",
    "# With time filter (last 365 days)\n",
    "await search(\"cybersecurity risks\", time_gate_days=365)\n",
    "\n",
    "# With source filter (only filings, not JSON facts)\n",
    "await search(\"cybersecurity risks\", source_types=[\"filing\", \"exhibit\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid search ready!\n"
     ]
    }
   ],
   "source": [
    "async def search(\n",
    "    query: str,\n",
    "    time_gate_days: int | None = None,\n",
    "    source_types: list[str] | None = None,\n",
    "    limit: int = 10,\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Hybrid search using RRF (Reciprocal Rank Fusion).\n",
    "\n",
    "    Combines semantic similarity and keyword matching.\n",
    "    \"\"\"\n",
    "    # Prepare query components\n",
    "    embedding = format_embedding(await text_to_embedding.eval_async(query))\n",
    "    keywords = extract_keywords(query)\n",
    "\n",
    "    # Build filter conditions\n",
    "    conditions = []\n",
    "    if time_gate_days:\n",
    "        conditions.append(\n",
    "            f\"filing_date >= DATE_SUB(CURRENT_DATE(), INTERVAL {time_gate_days} DAY)\"\n",
    "        )\n",
    "    if source_types:\n",
    "        conditions.append(f\"source_type IN ({format_list(source_types)})\")\n",
    "    where = build_where(conditions)\n",
    "\n",
    "    # RRF hybrid search\n",
    "    sql = f\"\"\"\n",
    "    WITH\n",
    "    semantic AS (\n",
    "        SELECT chunk_id, doc_filename, cik, filing_date, source_type, text, topics,\n",
    "               ROW_NUMBER() OVER (ORDER BY l2_distance(embedding, {embedding})) AS rank\n",
    "        FROM {TABLE} WHERE {where}\n",
    "    ),\n",
    "    lexical AS (\n",
    "        SELECT chunk_id,\n",
    "               ROW_NUMBER() OVER (ORDER BY CASE WHEN text MATCH_ANY '{keywords}' THEN 0 ELSE 1 END) AS rank\n",
    "        FROM {TABLE} WHERE {where}\n",
    "    )\n",
    "    SELECT s.*, l.rank AS lex_rank,\n",
    "           1.0/(60 + s.rank) + 1.0/(60 + l.rank) AS score\n",
    "    FROM semantic s JOIN lexical l USING (chunk_id)\n",
    "    ORDER BY score DESC LIMIT {limit}\n",
    "    \"\"\"\n",
    "\n",
    "    return [\n",
    "        {\n",
    "            \"doc_filename\": r[1],\n",
    "            \"cik\": r[2],\n",
    "            \"filing_date\": str(r[3]) if r[3] else None,\n",
    "            \"source_type\": r[4],\n",
    "            \"text\": r[5],\n",
    "            \"topics\": r[6] or [],\n",
    "            \"sem_rank\": r[7],\n",
    "            \"lex_rank\": r[8],\n",
    "            \"rrf_score\": float(r[9]),\n",
    "        }\n",
    "        for r in await doris_query(DORIS_CONFIG, sql)\n",
    "    ]\n",
    "\n",
    "\n",
    "print(\"Hybrid search ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Mode 2: Lexical Search\n",
    "\n",
    "**When to use**: Exact terms, acronyms, regulatory codes, or when you need precise keyword matching.\n",
    "\n",
    "```python\n",
    "# Find exact mentions of \"supply chain\" as a phrase\n",
    "await search_lexical(\"supply chain\", match_type=\"phrase\")\n",
    "\n",
    "# Find chunks with ANY of these terms (OR)\n",
    "await search_lexical(\"supply chain\", match_type=\"any\")\n",
    "\n",
    "# Find chunks with ALL of these terms (AND)\n",
    "await search_lexical(\"supply chain\", match_type=\"all\")\n",
    "```\n",
    "\n",
    "**Doris Inverted Index Operators**:\n",
    "\n",
    "| Operator | SQL | Behavior | Strictness |\n",
    "|----------|-----|----------|------------|\n",
    "| `MATCH_ANY` | `text MATCH_ANY 'supply chain'` | supply OR chain | Broadest |\n",
    "| `MATCH_ALL` | `text MATCH_ALL 'supply chain'` | supply AND chain | Stricter |\n",
    "| `MATCH_PHRASE` | `text MATCH_PHRASE 'supply chain'` | Exact phrase | Strictest |\n",
    "\n",
    "**Relevance Ranking**: Doris provides `SCORE()` — a BM25-based relevance score computed during full-text matching. We use `ORDER BY SCORE() DESC` to return the most relevant results first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical search ready!\n"
     ]
    }
   ],
   "source": [
    "async def search_lexical(\n",
    "    query: str,\n",
    "    match_type: str = \"any\",  # \"any\", \"all\", or \"phrase\"\n",
    "    source_types: list[str] | None = None,\n",
    "    limit: int = 10,\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Keyword search using Doris inverted index.\n",
    "\n",
    "    match_type: \"any\" (OR), \"all\" (AND), \"phrase\" (exact sequence)\n",
    "\n",
    "    Results are ranked by BM25 relevance using Doris SCORE().\n",
    "    \"\"\"\n",
    "    keywords = extract_keywords(query)\n",
    "    match_op = {\"any\": \"MATCH_ANY\", \"all\": \"MATCH_ALL\", \"phrase\": \"MATCH_PHRASE\"}[\n",
    "        match_type\n",
    "    ]\n",
    "\n",
    "    # Build conditions\n",
    "    conditions = [f\"text {match_op} '{keywords}'\"]\n",
    "    if source_types:\n",
    "        conditions.append(f\"source_type IN ({format_list(source_types)})\")\n",
    "\n",
    "    sql = f\"\"\"\n",
    "    SELECT doc_filename, cik, filing_date, source_type, text, topics, SCORE() AS score\n",
    "    FROM {TABLE}\n",
    "    WHERE {build_where(conditions)}\n",
    "    ORDER BY score DESC\n",
    "    LIMIT {limit}\n",
    "    \"\"\"\n",
    "\n",
    "    return [\n",
    "        {\n",
    "            \"doc_filename\": r[0],\n",
    "            \"cik\": r[1],\n",
    "            \"filing_date\": str(r[2]) if r[2] else None,\n",
    "            \"source_type\": r[3],\n",
    "            \"text\": r[4],\n",
    "            \"topics\": r[5] or [],\n",
    "            \"score\": float(r[6]),\n",
    "        }\n",
    "        for r in await doris_query(DORIS_CONFIG, sql)\n",
    "    ]\n",
    "\n",
    "\n",
    "print(\"Lexical search ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MATCH_ANY (OR): Finds chunks with 'supply' OR 'chain'\n",
      "----------------------------------------------------------------------\n",
      "1. [score=8.7] APPLE INC. FORM 10-K ANNUAL REPORT\n",
      "ITEM 1A. RISK FACTORS\n",
      "\n",
      "CYBERSECURITY RISKS\n",
      "Th...\n",
      "2. [score=8.3] The Company has a large, global business with sales outside the U.S. representin...\n",
      "3. [score=7.9] Major public health issues, including pandemics such as the COVID-19 pandemic, h...\n",
      "\n",
      "======================================================================\n",
      "MATCH_ALL (AND): Finds chunks with BOTH 'supply' AND 'chain'\n",
      "----------------------------------------------------------------------\n",
      "1. [score=8.7] APPLE INC. FORM 10-K ANNUAL REPORT\n",
      "ITEM 1A. RISK FACTORS\n",
      "\n",
      "CYBERSECURITY RISKS\n",
      "Th...\n",
      "2. [score=8.3] The Company has a large, global business with sales outside the U.S. representin...\n",
      "3. [score=7.9] Major public health issues, including pandemics such as the COVID-19 pandemic, h...\n",
      "\n",
      "======================================================================\n",
      "MATCH_PHRASE (exact): Finds chunks with exact phrase 'supply chain'\n",
      "----------------------------------------------------------------------\n",
      "1. [score=8.7] APPLE INC. FORM 10-K ANNUAL REPORT\n",
      "ITEM 1A. RISK FACTORS\n",
      "\n",
      "CYBERSECURITY RISKS\n",
      "Th...\n",
      "2. [score=8.3] The Company has a large, global business with sales outside the U.S. representin...\n",
      "3. [score=7.9] Major public health issues, including pandemics such as the COVID-19 pandemic, h...\n",
      "\n",
      "Results ranked by BM25 score via Doris SCORE().\n",
      "Notice: ANY (broadest) → ALL (stricter) → PHRASE (strictest).\n"
     ]
    }
   ],
   "source": [
    "# Compare MATCH_ANY (OR) vs MATCH_ALL (AND) vs MATCH_PHRASE (exact)\n",
    "# Using \"supply chain\" — a phrase that demonstrates all three operators well.\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MATCH_ANY (OR): Finds chunks with 'supply' OR 'chain'\")\n",
    "print(\"-\" * 70)\n",
    "results = await search_lexical(\"supply chain\", match_type=\"any\", limit=3)\n",
    "for i, r in enumerate(results, 1):\n",
    "    print(f\"{i}. [score={r['score']:.1f}] {r['text'][:80]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MATCH_ALL (AND): Finds chunks with BOTH 'supply' AND 'chain'\")\n",
    "print(\"-\" * 70)\n",
    "results = await search_lexical(\"supply chain\", match_type=\"all\", limit=3)\n",
    "for i, r in enumerate(results, 1):\n",
    "    print(f\"{i}. [score={r['score']:.1f}] {r['text'][:80]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MATCH_PHRASE (exact): Finds chunks with exact phrase 'supply chain'\")\n",
    "print(\"-\" * 70)\n",
    "results = await search_lexical(\"supply chain\", match_type=\"phrase\", limit=3)\n",
    "for i, r in enumerate(results, 1):\n",
    "    print(f\"{i}. [score={r['score']:.1f}] {r['text'][:80]}...\")\n",
    "\n",
    "print(\"\\nResults ranked by BM25 score via Doris SCORE().\")\n",
    "print(\"Notice: ANY (broadest) → ALL (stricter) → PHRASE (strictest).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Hybrid Search with RRF Scoring\n",
    "\n",
    "Notice how RRF combines semantic rank and lexical rank. Documents scoring well on BOTH get boosted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'cybersecurity risks and data breach'\n",
      "===========================================================================\n",
      "#   RRF Score    Sem Rank   Lex Rank   Source     File\n",
      "---------------------------------------------------------------------------\n",
      "1   0.0328       #1        #1        filing     0000789019_2025-07-30_10-K.txt\n",
      "2   0.0313       #6        #2        filing     0000320193_2025-10-31_10-K.txt\n",
      "3   0.0310       #4        #5        filing     0000320193_2025-10-31_10-K.txt\n",
      "4   0.0298       #2        #13       filing     0000320193_2025-10-31_10-K.txt\n",
      "5   0.0295       #5        #11       filing     0000789019_2025-07-30_10-K.txt\n",
      "\n",
      "===========================================================================\n",
      "How to read: Lower rank = better. RRF boosts docs that rank well on BOTH signals.\n",
      "\n",
      "Top result preview:\n",
      "  Topics: [\"RISK:CYBER\",\"RISK:REGULATORY\"]\n",
      "  Text: Business\n",
      "3\n",
      "Information about our Executive Officers\n",
      "14\n",
      "Item 1A.\n",
      "Risk Factors\n",
      "16\n",
      "Item 1B.\n",
      "Unresolved Staff Comments\n",
      "30\n",
      "Item 1C.\n",
      "Cybersecurity\n",
      "30\n",
      "Item 2.\n",
      "Properties\n",
      "32\n",
      "Item 3.\n",
      "Legal Proceedings\n",
      "32\n",
      "Item ...\n"
     ]
    }
   ],
   "source": [
    "# Hybrid search: semantic + keyword combined via RRF\n",
    "results = await search(\n",
    "    \"cybersecurity risks and data breach\", time_gate_days=365, limit=5\n",
    ")\n",
    "\n",
    "print(\"Query: 'cybersecurity risks and data breach'\")\n",
    "print(\"=\" * 75)\n",
    "print(\n",
    "    f\"{'#':<3} {'RRF Score':<12} {'Sem Rank':<10} {'Lex Rank':<10} {'Source':<10} File\"\n",
    ")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for i, r in enumerate(results, 1):\n",
    "    print(\n",
    "        f\"{i:<3} {r['rrf_score']:.4f}       #{r['sem_rank']:<8} #{r['lex_rank']:<8} {r['source_type']:<10} {r['doc_filename'][:30]}\"\n",
    "    )\n",
    "\n",
    "print(\"\\n\" + \"=\" * 75)\n",
    "print(\n",
    "    \"How to read: Lower rank = better. RRF boosts docs that rank well on BOTH signals.\"\n",
    ")\n",
    "print(\"\\nTop result preview:\")\n",
    "if results:\n",
    "    r = results[0]\n",
    "    print(f\"  Topics: {r['topics']}\")\n",
    "    print(f\"  Text: {r['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Filter by Source Type\n",
    "\n",
    "Search only specific data formats using `source_types` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source filter: filing only\n",
      "============================================================\n",
      "1. [filing] 0000320193_2025-10-31_10-K.txt\n",
      "   The Company has historically experienced higher net sales in its first quarter compared to other qua...\n",
      "\n",
      "2. [filing] 0000320193_2025-10-31_10-K.txt\n",
      "   Markets and Distribution\n",
      "The Company’s customers are primarily in the consumer, small and mid-sized ...\n",
      "\n",
      "3. [filing] 0000789019_2025-07-30_10-K.txt\n",
      "   Indicate by check mark whether the registrant has filed a report on and attestation to its managemen...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Search ONLY in filings (after multi-source indexing, you can also filter by \"facts\" or \"exhibit\")\n",
    "print(\"Source filter: filing only\")\n",
    "print(\"=\" * 60)\n",
    "results = await search(\"revenue net income assets\", source_types=[\"filing\"], limit=3)\n",
    "for i, r in enumerate(results, 1):\n",
    "    print(f\"{i}. [{r['source_type']}] {r['doc_filename']}\")\n",
    "    print(f\"   {r['text'][:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source filter: filing, exhibit (skip JSON)\n",
      "============================================================\n",
      "1. [filing] 0000789019_2025-07-30_10-K.txt\n",
      "   Topics: [\"RISK:CYBER\",\"RISK:REGULATORY\"]\n",
      "\n",
      "2. [filing] 0000789019_2025-07-30_10-K.txt\n",
      "   Topics: [\"RISK:REGULATORY\"]\n",
      "\n",
      "3. [filing] 0000789019_2025-07-30_10-K.txt\n",
      "   Topics: [\"RISK:REGULATORY\"]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Search filings + exhibits (skip JSON facts)\n",
    "print(\"Source filter: filing, exhibit (skip JSON)\")\n",
    "print(\"=\" * 60)\n",
    "results = await search(\"risk disclosure\", source_types=[\"filing\", \"exhibit\"], limit=3)\n",
    "for i, r in enumerate(results, 1):\n",
    "    print(f\"{i}. [{r['source_type']}] {r['doc_filename'][:40]}\")\n",
    "    print(f\"   Topics: {r['topics']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Mode 3: Topic-Filtered Search\n",
    "\n",
    "**When to use**: You know the category/risk type and want to search within that subset.\n",
    "\n",
    "**How it works**: \n",
    "1. First, filter chunks by topic tags (using `json_contains()`)\n",
    "2. Then, rank by semantic similarity within that subset\n",
    "\n",
    "**Available Topics** (extracted by `extract_topics()` in `functions.py`):\n",
    "```\n",
    "RISK:CYBER      - Cybersecurity, data breaches, ransomware\n",
    "RISK:CLIMATE    - Climate change, emissions, sustainability  \n",
    "RISK:SUPPLY     - Supply chain, logistics, shortages\n",
    "RISK:REGULATORY - Compliance, SEC, regulations\n",
    "TOPIC:AI        - Artificial intelligence, machine learning\n",
    "TOPIC:CLOUD     - Cloud computing, AWS, Azure, SaaS\n",
    "TOPIC:FINANCIAL - Revenue, income, assets, cash flow\n",
    "```\n",
    "\n",
    "```python\n",
    "# Find AI-related content (semantic search within TOPIC:AI chunks)\n",
    "await search_by_topics(\"company strategy\", topics=[\"TOPIC:AI\"])\n",
    "\n",
    "# Find content with ANY of these topics (OR)\n",
    "await search_by_topics(\"company risks\", topics=[\"RISK:CYBER\", \"RISK:CLIMATE\"], match=\"any\")\n",
    "\n",
    "# Find content with ALL of these topics (AND) - more restrictive\n",
    "await search_by_topics(\"regulatory cyber\", topics=[\"RISK:CYBER\", \"RISK:REGULATORY\"], match=\"all\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic search ready!\n"
     ]
    }
   ],
   "source": [
    "async def search_by_topics(\n",
    "    query: str,\n",
    "    topics: list[str],\n",
    "    match: str = \"any\",  # \"any\" or \"all\"\n",
    "    source_types: list[str] | None = None,\n",
    "    limit: int = 10,\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Semantic search filtered by topic tags.\n",
    "\n",
    "    match=\"any\": Has ANY of these topics (OR)\n",
    "    match=\"all\": Has ALL of these topics (AND)\n",
    "\n",
    "    Note: topics column is stored as JSON in Doris, so we use json_contains()\n",
    "    instead of array_contains() for filtering.\n",
    "    \"\"\"\n",
    "    embedding = format_embedding(await text_to_embedding.eval_async(query))\n",
    "\n",
    "    # Build topic filter using json_contains (topics is JSON type in Doris)\n",
    "    if match == \"any\":\n",
    "        # OR: any of the topics\n",
    "        topic_conditions = [f\"json_contains(topics, '\\\"{t}\\\"')\" for t in topics]\n",
    "        conditions = [\"(\" + \" OR \".join(topic_conditions) + \")\"]\n",
    "    else:\n",
    "        # AND: all of the topics\n",
    "        conditions = [f\"json_contains(topics, '\\\"{t}\\\"')\" for t in topics]\n",
    "\n",
    "    if source_types:\n",
    "        conditions.append(f\"source_type IN ({format_list(source_types)})\")\n",
    "\n",
    "    sql = f\"\"\"\n",
    "    SELECT doc_filename, cik, filing_date, source_type, text, topics,\n",
    "           l2_distance(embedding, {embedding}) AS score\n",
    "    FROM {TABLE}\n",
    "    WHERE {build_where(conditions)}\n",
    "    ORDER BY score LIMIT {limit}\n",
    "    \"\"\"\n",
    "\n",
    "    return [\n",
    "        {\n",
    "            \"doc_filename\": r[0],\n",
    "            \"cik\": r[1],\n",
    "            \"filing_date\": str(r[2]) if r[2] else None,\n",
    "            \"source_type\": r[3],\n",
    "            \"text\": r[4],\n",
    "            \"topics\": r[5] or [],\n",
    "            \"score\": float(r[6]),\n",
    "        }\n",
    "        for r in await doris_query(DORIS_CONFIG, sql)\n",
    "    ]\n",
    "\n",
    "\n",
    "print(\"Topic search ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic filter: TOPIC:AI\n",
      "============================================================\n",
      "1. [filing] 0000320193_2024-11-01_10-K.txt\n",
      "   Score: 1.217 | Topics: [\"RISK:CYBER\",\"RISK:CLIMATE\",\"RISK:SUPPLY\",\"RISK:REGULATORY\",\"TOPIC:AI\"]\n",
      "   APPLE INC. FORM 10-K ANNUAL REPORT\n",
      "ITEM 1A. RISK FACTORS\n",
      "\n",
      "CYBERSECURITY RISKS\n",
      "Th...\n",
      "\n",
      "2. [filing] 0000789019_2025-07-30_10-K.txt\n",
      "   Score: 1.232 | Topics: [\"RISK:REGULATORY\",\"TOPIC:AI\",\"TOPIC:CLOUD\"]\n",
      "   PART\n",
      "I\n",
      "ITEM 1. B\n",
      "USINESS\n",
      "GENERAL\n",
      "Microsoft is a technology company committed to ...\n",
      "\n",
      "3. [filing] 0000789019_2024-10-15_10-K.txt\n",
      "   Score: 1.289 | Topics: [\"RISK:CYBER\",\"RISK:REGULATORY\",\"TOPIC:AI\",\"TOPIC:CLOUD\"]\n",
      "   MICROSOFT CORPORATION FORM 10-K ANNUAL REPORT\n",
      "ITEM 1A. RISK FACTORS\n",
      "\n",
      "CLOUD INFRA...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Topic filter: Only chunks tagged with TOPIC:AI\n",
    "print(\"Topic filter: TOPIC:AI\")\n",
    "print(\"=\" * 60)\n",
    "results = await search_by_topics(\n",
    "    \"company strategy investments\", topics=[\"TOPIC:AI\"], limit=3\n",
    ")\n",
    "\n",
    "for i, r in enumerate(results, 1):\n",
    "    print(f\"{i}. [{r['source_type']}] {r['doc_filename']}\")\n",
    "    print(f\"   Score: {r['score']:.3f} | Topics: {r['topics']}\")\n",
    "    print(f\"   {r['text'][:80]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Mode 4: Portfolio Search\n",
    "\n",
    "**When to use**: Compare how multiple companies discuss the same topic.\n",
    "\n",
    "**How it works**:\n",
    "1. Filter to specified companies (by CIK)\n",
    "2. Rank by semantic similarity\n",
    "3. Use `PARTITION BY cik` to get top-K results PER company\n",
    "\n",
    "This ensures each company is represented in results, even if one company has more relevant content.\n",
    "\n",
    "```python\n",
    "# Compare cybersecurity across 3 companies (top 2 chunks each)\n",
    "await search_portfolio(\n",
    "    \"cybersecurity investments\",\n",
    "    ciks=[\"0000320193\", \"0000789019\", \"0000019617\"],  # Apple, Microsoft, JPMorgan\n",
    "    top_k=2\n",
    ")\n",
    "\n",
    "# Compare only within filings (skip JSON facts and PDF exhibits)\n",
    "await search_portfolio(\n",
    "    \"AI strategy\",\n",
    "    ciks=[\"0000320193\", \"0000789019\"],\n",
    "    source_types=[\"filing\"],\n",
    "    top_k=3\n",
    ")\n",
    "```\n",
    "\n",
    "**Common CIKs** (for reference):\n",
    "```\n",
    "0000320193  Apple\n",
    "0000789019  Microsoft\n",
    "0000019617  JPMorgan Chase\n",
    "0001018724  Amazon\n",
    "0001652044  Alphabet (Google)\n",
    "0001326801  Meta (Facebook)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portfolio search ready!\n"
     ]
    }
   ],
   "source": [
    "async def search_portfolio(\n",
    "    query: str,\n",
    "    ciks: list[str],\n",
    "    source_types: list[str] | None = None,\n",
    "    top_k: int = 2,\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Get top results per company for cross-company comparison.\n",
    "\n",
    "    Uses PARTITION BY to rank within each company.\n",
    "    \"\"\"\n",
    "    embedding = format_embedding(await text_to_embedding.eval_async(query))\n",
    "\n",
    "    conditions = [f\"cik IN ({format_list(ciks)})\"]\n",
    "    if source_types:\n",
    "        conditions.append(f\"source_type IN ({format_list(source_types)})\")\n",
    "\n",
    "    sql = f\"\"\"\n",
    "    WITH ranked AS (\n",
    "        SELECT cik, doc_filename, source_type, text, topics,\n",
    "               l2_distance(embedding, {embedding}) AS score,\n",
    "               ROW_NUMBER() OVER (PARTITION BY cik ORDER BY l2_distance(embedding, {embedding})) AS rank\n",
    "        FROM {TABLE}\n",
    "        WHERE {build_where(conditions)}\n",
    "    )\n",
    "    SELECT cik, doc_filename, source_type, text, topics, score\n",
    "    FROM ranked WHERE rank <= {top_k}\n",
    "    ORDER BY cik, score\n",
    "    \"\"\"\n",
    "\n",
    "    return [\n",
    "        {\n",
    "            \"cik\": r[0],\n",
    "            \"doc_filename\": r[1],\n",
    "            \"source_type\": r[2],\n",
    "            \"text\": r[3],\n",
    "            \"topics\": r[4] or [],\n",
    "            \"score\": float(r[5]),\n",
    "        }\n",
    "        for r in await doris_query(DORIS_CONFIG, sql)\n",
    "    ]\n",
    "\n",
    "\n",
    "print(\"Portfolio search ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portfolio Comparison: 'cybersecurity investments'\n",
      "=================================================================\n",
      "\n",
      "JPMorgan (0000019617) [filing]\n",
      "  Score: 1.072 | Topics: [\"RISK:CYBER\",\"RISK:CLIMATE\",\"RISK:REGULATORY\"]\n",
      "  JPMORGAN CHASE & CO. FORM 10-K ANNUAL REPORT\n",
      "ITEM 1A. RISK FACTORS\n",
      "\n",
      "CREDIT RISK\n",
      "The Company faces significant credit ris...\n",
      "\n",
      "Apple (0000320193) [filing]\n",
      "  Score: 1.018 | Topics: [\"RISK:CYBER\",\"RISK:CLIMATE\",\"RISK:SUPPLY\",\"RISK:REGULATORY\",\"TOPIC:AI\"]\n",
      "  APPLE INC. FORM 10-K ANNUAL REPORT\n",
      "ITEM 1A. RISK FACTORS\n",
      "\n",
      "CYBERSECURITY RISKS\n",
      "The Company faces significant cybersecurit...\n",
      "\n",
      "Microsoft (0000789019) [filing]\n",
      "  Score: 1.060 | Topics: [\"RISK:CYBER\",\"RISK:REGULATORY\",\"TOPIC:AI\",\"TOPIC:CLOUD\"]\n",
      "  MICROSOFT CORPORATION FORM 10-K ANNUAL REPORT\n",
      "ITEM 1A. RISK FACTORS\n",
      "\n",
      "CLOUD INFRASTRUCTURE\n",
      "Our Azure cloud platform faces...\n"
     ]
    }
   ],
   "source": [
    "# Portfolio comparison: Top result per company\n",
    "companies = {\"0000320193\": \"Apple\", \"0000789019\": \"Microsoft\", \"0000019617\": \"JPMorgan\"}\n",
    "\n",
    "print(\"Portfolio Comparison: 'cybersecurity investments'\")\n",
    "print(\"=\" * 65)\n",
    "results = await search_portfolio(\n",
    "    \"cybersecurity investments\", ciks=list(companies.keys()), top_k=1\n",
    ")\n",
    "\n",
    "for r in results:\n",
    "    name = companies.get(r[\"cik\"], r[\"cik\"])\n",
    "    print(f\"\\n{name} ({r['cik']}) [{r['source_type']}]\")\n",
    "    print(f\"  Score: {r['score']:.3f} | Topics: {r['topics']}\")\n",
    "    print(f\"  {r['text'][:120]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 7: Access Control\n",
    "\n",
    "Apache Doris provides **row-level** and **column-level** security for compliance requirements.\n",
    "\n",
    "### Column-Level Security\n",
    "\n",
    "Hide sensitive columns from certain users (e.g., analysts shouldn't see raw embeddings):\n",
    "\n",
    "```sql\n",
    "-- Create analyst role with limited column access\n",
    "GRANT SELECT_PRIV (\n",
    "    chunk_id, doc_filename, cik, filing_date, text, topics\n",
    "    -- embedding column NOT included\n",
    ") ON sec_analytics.filing_chunks TO 'analyst'@'%';\n",
    "```\n",
    "\n",
    "### Row-Level Security\n",
    "\n",
    "Restrict users to specific companies or data subsets:\n",
    "\n",
    "```sql\n",
    "-- Tech analyst can only see Apple and Microsoft filings\n",
    "CREATE ROW POLICY tech_analyst_policy ON filing_chunks\n",
    "AS RESTRICTIVE TO 'tech_analyst'@'%'\n",
    "USING (cik IN ('0000320193', '0000789019'));\n",
    "\n",
    "-- Compliance officer can only see regulatory-tagged content\n",
    "-- Note: topics is JSON type, so use json_contains()\n",
    "CREATE ROW POLICY compliance_policy ON filing_chunks\n",
    "AS RESTRICTIVE TO 'compliance'@'%'\n",
    "USING (json_contains(topics, '\"RISK:REGULATORY\"'));\n",
    "\n",
    "-- Time-based restriction (only filings from 2024+)\n",
    "CREATE ROW POLICY recent_only_policy ON filing_chunks\n",
    "AS RESTRICTIVE TO 'junior_analyst'@'%'\n",
    "USING (filing_date >= '2024-01-01');\n",
    "```\n",
    "\n",
    "### Source Type Restrictions\n",
    "\n",
    "Limit access to specific data formats:\n",
    "\n",
    "```sql\n",
    "-- User can only search 10-K filings, not PDF exhibits\n",
    "CREATE ROW POLICY filing_only_policy ON filing_chunks\n",
    "AS RESTRICTIVE TO 'filing_viewer'@'%'\n",
    "USING (source_type = 'filing');\n",
    "```\n",
    "\n",
    "**Key point**: These policies are enforced at the database level — all queries (including our search functions) automatically respect them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Summary\n\nYou've built a production-ready SEC filing search system with:\n\n| Feature | Implementation |\n|---------|----------------|\n| **Multi-format ingestion** | TXT (filings) + JSON (API) + PDF (exhibits) |\n| **Unified search** | Single collector, `source_type` field for filtering |\n| **Semantic search** | SentenceTransformer embeddings + HNSW index |\n| **Keyword search** | Doris inverted index (MATCH_ANY/ALL/PHRASE) |\n| **Hybrid ranking** | RRF fusion (no weight tuning needed) |\n| **Temporal awareness** | Time gating via WHERE clause |\n| **Category filtering** | topics[] JSON array with `json_contains()` |\n| **Portfolio analysis** | `PARTITION BY` for per-company results |\n| **Access control** | Row/column-level security via `CREATE ROW POLICY` |\n| **Incremental updates** | CocoIndex caching |\n| **Audit trail** | doc_filename + location + source_type lineage |\n\n### Search API\n\n```python\n# Hybrid search (semantic + keyword)\nawait search(\"cybersecurity risks\", time_gate_days=365)\n\n# Lexical search (keyword only)\nawait search_lexical(\"GDPR\", match_type=\"phrase\")\n\n# Topic-filtered search\nawait search_by_topics(\"risks\", topics=[\"RISK:CYBER\"])\n\n# Portfolio comparison (top-k per company)\nawait search_portfolio(\"cybersecurity\", ciks=[\"0000320193\", \"0000789019\"])\n```\n\n### Project Files\n\n| File | Purpose |\n|------|---------|\n| `functions.py` | CocoIndex transformation functions (ETL) |\n| `search.py` | Low-level Doris SQL helpers |\n| `download.py` | Sample data creation |\n\n### Next Steps\n\n1. **View lineage**: `cocoindex server -ci main.py`\n2. **Add more data**: Extend `download.py` with additional companies\n\n### Resources\n\n- [CocoIndex Documentation](https://cocoindex.io/docs)\n- [Apache Doris Vector Search](https://doris.apache.org/docs)\n- [SEC EDGAR API](https://www.sec.gov/developer)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
